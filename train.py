# -*- coding: utf-8 -*-
"""tarin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H0XPgkbtriKUH85xGAnfjXPht4chwtzT
"""

import torch
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time

from teacher_model import create_teacher_model, get_teacher_config
from student_model import create_student_model, count_parameters
from knowledge_distillation import distill_knowledge, evaluate_model

def load_cifar10_data():
    """Load and prepare CIFAR-10 dataset"""
    # Data augmentation for training
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                           (0.2023, 0.1994, 0.2010))
    ])

    # No augmentation for validation
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465),
                           (0.2023, 0.1994, 0.2010))
    ])

    # Load datasets
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                          download=True, transform=transform_train)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                         download=True, transform=transform_test)

    # Create data loaders
    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)
    testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)

    return trainloader, testloader

def train_teacher(teacher_model, trainloader, testloader, device='cuda'):
    """Train the teacher model"""
    print("Training Teacher Model (ResNet-18)...")

    config = get_teacher_config()
    optimizer = optim.SGD(teacher_model.parameters(),
                         lr=config['lr'],
                         momentum=config['momentum'],
                         weight_decay=config['weight_decay'])

    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, config['epochs'])
    criterion = torch.nn.CrossEntropyLoss()

    train_accuracies = []
    val_accuracies = []

    for epoch in range(config['epochs']):
        teacher_model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, labels) in enumerate(trainloader):
            data, labels = data.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = teacher_model(data)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_acc = correct / total
        val_acc = evaluate_model(teacher_model, testloader, device)

        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)

        scheduler.step()

        if epoch % 20 == 0:
            print(f'Epoch {epoch}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')

    return train_accuracies, val_accuracies

def main():
    """Main training pipeline"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load data
    trainloader, testloader = load_cifar10_data()

    # Create models
    teacher_model = create_teacher_model(device)
    student_model = create_student_model(device)

    print(f"Teacher parameters: {count_parameters(teacher_model):,}")
    print(f"Student parameters: {count_parameters(student_model):,}")

    # Train teacher model
    teacher_train_acc, teacher_val_acc = train_teacher(
        teacher_model, trainloader, testloader, device)

    print(f"Teacher final accuracy: {teacher_val_acc[-1]:.4f}")

    # Save teacher model
    torch.save(teacher_model.state_dict(), 'teacher_model.pth')

    # Knowledge distillation
    print("\nStarting Knowledge Distillation...")
    student_losses, student_accuracies = distill_knowledge(
        teacher_model, student_model, trainloader, testloader, device)

    print(f"Student final accuracy: {student_accuracies[-1]:.4f}")

    # Save student model
    torch.save(student_model.state_dict(), 'student_model.pth')

    # Performance comparison
    teacher_final_acc = teacher_val_acc[-1]
    student_final_acc = student_accuracies[-1]

    print("\n" + "="*50)
    print("FINAL RESULTS:")
    print(f"Teacher Accuracy: {teacher_final_acc:.4f}")
    print(f"Student Accuracy: {student_final_acc:.4f}")
    print(f"Accuracy Drop: {teacher_final_acc - student_final_acc:.4f}")
    print(f"Parameter Reduction: {count_parameters(teacher_model) / count_parameters(student_model):.1f}x")
    print("="*50)

if __name__ == "__main__":
    main()